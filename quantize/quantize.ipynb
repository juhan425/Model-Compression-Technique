{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X3yAheTVHy6"
      },
      "source": [
        "# **1.Import Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bY5BfhsJVBXd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models.feature_extraction as feature_extraction\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import copy\n",
        "\n",
        "no_cuda = False\n",
        "use_gpu = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_gpu else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWzRKQJ5VKIp"
      },
      "source": [
        "# **2.Load Fashion MNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdaU4h2sVGCi",
        "outputId": "8cc1cefe-07fe-4d64-d428-e471a67e779a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:03<00:00, 8210341.91it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 137771.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2550953.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 17918902.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#Dataset\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform, download=False)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform, download=False)\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd0z1xEWVNMo"
      },
      "source": [
        "# **3.Create and train a NN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fxfpJb5XVtCD"
      },
      "outputs": [],
      "source": [
        "class ToyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.nn1 = nn.Linear(28*28, 120)\n",
        "    self.nn2 = nn.Linear(120, 84)\n",
        "    self.nn3 = nn.Linear(84, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28) #transform 28*28 figure to 784 vector\n",
        "    x = F.relu(self.nn1(x))\n",
        "    x = F.relu(self.nn2(x))\n",
        "    x = self.nn3(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH5gJgf0Vx11",
        "outputId": "9a24fac0-4481-4aab-8ddb-9e84b1f06601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 120]          94,200\n",
            "            Linear-2                   [-1, 84]          10,164\n",
            "            Linear-3                   [-1, 10]             850\n",
            "================================================================\n",
            "Total params: 105,214\n",
            "Trainable params: 105,214\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 0.41\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "FP32_model = ToyModel().to(device)\n",
        "summary(FP32_model,(1,28,28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5n28iqPfWNz6"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  #Set the model to train mode\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    if use_gpu:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "    optimizer.zero_grad()\n",
        "    #forward\n",
        "    pred = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    #backward\n",
        "    loss.backward()\n",
        "\n",
        "    #optimize\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  #set model to evaluate mode\n",
        "  model.eval()\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      if use_gpu:\n",
        "        x, y = x.cuda(), y.cuda()\n",
        "      pred = model(x)\n",
        "      test_loss = loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item() #calculate accuracy\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4QeI-kqVzNN",
        "outputId": "60f8a229-ebe2-4f7a-c169-8725be1702f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToyModel(\n",
              "  (nn1): Linear(in_features=784, out_features=120, bias=True)\n",
              "  (nn2): Linear(in_features=120, out_features=84, bias=True)\n",
              "  (nn3): Linear(in_features=84, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "epochs = 3\n",
        "loss_fn = nn.CrossEntropyLoss() #define loss function\n",
        "optimizer = torch.optim.SGD(FP32_model.parameters(), lr=learning_rate, momentum=0.9)  #define optimizer\n",
        "\n",
        "FP32_model.to(device) #let model on GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdawCUguWQ39",
        "outputId": "fc72d917-b01e-48b5-b14f-8a5dfc90386c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.320946  [   32/60000]\n",
            "loss: 2.052694  [ 3232/60000]\n",
            "loss: 1.597480  [ 6432/60000]\n",
            "loss: 1.057126  [ 9632/60000]\n",
            "loss: 0.928133  [12832/60000]\n",
            "loss: 0.859645  [16032/60000]\n",
            "loss: 0.773342  [19232/60000]\n",
            "loss: 0.519602  [22432/60000]\n",
            "loss: 0.759602  [25632/60000]\n",
            "loss: 0.703240  [28832/60000]\n",
            "loss: 0.745862  [32032/60000]\n",
            "loss: 0.688412  [35232/60000]\n",
            "loss: 0.894060  [38432/60000]\n",
            "loss: 0.602049  [41632/60000]\n",
            "loss: 0.372009  [44832/60000]\n",
            "loss: 0.511063  [48032/60000]\n",
            "loss: 0.751946  [51232/60000]\n",
            "loss: 0.630018  [54432/60000]\n",
            "loss: 0.534947  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 0.001033 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.596310  [   32/60000]\n",
            "loss: 0.421549  [ 3232/60000]\n",
            "loss: 0.378936  [ 6432/60000]\n",
            "loss: 0.402951  [ 9632/60000]\n",
            "loss: 0.675997  [12832/60000]\n",
            "loss: 0.705559  [16032/60000]\n",
            "loss: 0.357886  [19232/60000]\n",
            "loss: 0.431919  [22432/60000]\n",
            "loss: 0.384970  [25632/60000]\n",
            "loss: 0.435442  [28832/60000]\n",
            "loss: 0.369472  [32032/60000]\n",
            "loss: 0.345525  [35232/60000]\n",
            "loss: 0.204737  [38432/60000]\n",
            "loss: 0.373234  [41632/60000]\n",
            "loss: 0.812224  [44832/60000]\n",
            "loss: 0.362736  [48032/60000]\n",
            "loss: 0.353385  [51232/60000]\n",
            "loss: 0.422949  [54432/60000]\n",
            "loss: 0.419974  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 82.5%, Avg loss: 0.000884 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.273596  [   32/60000]\n",
            "loss: 0.701215  [ 3232/60000]\n",
            "loss: 0.310890  [ 6432/60000]\n",
            "loss: 0.676267  [ 9632/60000]\n",
            "loss: 0.552811  [12832/60000]\n",
            "loss: 0.618908  [16032/60000]\n",
            "loss: 0.297826  [19232/60000]\n",
            "loss: 0.438471  [22432/60000]\n",
            "loss: 0.538053  [25632/60000]\n",
            "loss: 0.731164  [28832/60000]\n",
            "loss: 0.290846  [32032/60000]\n",
            "loss: 0.329969  [35232/60000]\n",
            "loss: 0.525838  [38432/60000]\n",
            "loss: 0.638537  [41632/60000]\n",
            "loss: 0.310415  [44832/60000]\n",
            "loss: 0.211497  [48032/60000]\n",
            "loss: 0.513588  [51232/60000]\n",
            "loss: 0.422844  [54432/60000]\n",
            "loss: 0.660383  [57632/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.7%, Avg loss: 0.000831 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(epochs):\n",
        "  print(f\"Epoch {i+1}\\n-------------------------------\")\n",
        "  train_loop(train_loader, FP32_model, loss_fn, optimizer)\n",
        "  test_loop(test_loader, FP32_model, loss_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8tQ69OJ9stN"
      },
      "source": [
        "# **4. Quantize**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFkGGLz-laW"
      },
      "source": [
        "Use normal, clip scale and zero point algorithms and quantize the tensor based on the calculated s and z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e0U9OtUOTfwE"
      },
      "outputs": [],
      "source": [
        "def Calculate_scale_zero_point(x, mode=\"normal\"):\n",
        "  if mode == \"normal\":\n",
        "    q_max = 127\n",
        "    q_min = -128\n",
        "    max = torch.max(x)\n",
        "    min = torch.min(x)\n",
        "    scale = (max - min)/(q_max - q_min)\n",
        "    zero_point = round(float(q_min - (min / scale)))\n",
        "\n",
        "  elif mode == \"clip\":\n",
        "    q_max = 255\n",
        "    q_min = -256\n",
        "    max = torch.max(x)\n",
        "    min = torch.min(x)\n",
        "    scale = (max - min)/(q_max - q_min)\n",
        "    zero_point = round(float(q_min - (min / scale)))\n",
        "\n",
        "  return scale, zero_point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cgGxJQXVvuuA"
      },
      "outputs": [],
      "source": [
        "class Quantize_per_tensor(nn.Module):\n",
        "  def __init__(self, x, scale, zero_point, mode=\"normal\"):\n",
        "    super().__init__()\n",
        "    self.tensor = x\n",
        "    self.scale = scale\n",
        "    self.zero_point = zero_point\n",
        "    self._quantize(mode)\n",
        "\n",
        "  def repr(self):\n",
        "    return self.qtensor\n",
        "\n",
        "  def int_repr(self):\n",
        "    return self.qtensor_int\n",
        "\n",
        "  def _get_scale_zero(self):\n",
        "    return self.scale, self.zero_point\n",
        "\n",
        "  def _quantize(self, mode):\n",
        "    if mode == \"normal\":\n",
        "      self.qtensor_int = torch.round(self.tensor / self.scale + self.zero_point)\n",
        "      self.qtensor = (self.qtensor_int - self.zero_point) * self.scale \n",
        "\n",
        "    elif mode == \"clip\":\n",
        "      self.qtensor_int = torch.round(self.tensor / self.scale + self.zero_point)\n",
        "      self.qtensor_int = torch.clamp(self.qtensor_int, min = -256, max = 255)\n",
        "      self.qtensor = (self.qtensor_int - self.zero_point) * self.scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9KPMGRRJ6LJe"
      },
      "outputs": [],
      "source": [
        "class QuantizedLinear(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point, mode):\n",
        "    super(QuantizedLinear, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.mode = mode\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.matmul(x, self.weight.t())\n",
        "    output = Quantize_per_tensor(x, self.scale, self.zero_point, mode=self.mode).repr() + self.bias\n",
        "\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    s, z = Calculate_scale_zero_point(weight)\n",
        "    qweight = Quantize_per_tensor(weight, s, z, mode=self.mode)\n",
        "    return qweight.repr()\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinear(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'\n",
        "\n",
        "class QuantizedLinearReLU(nn.Module):\n",
        "  def __init__(self, in_features, out_features, weight, bias, scale, zero_point, mode):\n",
        "    super(QuantizedLinearReLU, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.mode = mode\n",
        "    self.scale, self.zero_point = scale, zero_point\n",
        "    self.weight = self._weight_quantize(weight)\n",
        "    self.bias = bias\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.matmul(x, self.weight.t())\n",
        "    output = Quantize_per_tensor(x, self.scale, self.zero_point, mode=self.mode).repr() + self.bias\n",
        "    output = F.relu(output)\n",
        "    return output\n",
        "\n",
        "  def _weight_quantize(self, weight):\n",
        "    s, z = Calculate_scale_zero_point(weight)\n",
        "    qweight = Quantize_per_tensor(weight, s, z, mode=self.mode)\n",
        "    return qweight.repr()\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'QuantizedLinearReLU(in_features={self.in_features}, out_features={self.out_features}, scale={self.scale}, zero_point={self.zero_point})'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DAGugFzu6zcv"
      },
      "outputs": [],
      "source": [
        "class QuantizedModel(nn.Module):\n",
        "  def __init__(self, model, scale, zero_point, mode=\"normal\"):\n",
        "    super(QuantizedModel, self).__init__()\n",
        "    self.weight_dic = []\n",
        "    self.bias_dic = []\n",
        "    self.scale, self.zero_point = scale, zero_point #Scale and zero point of all layer\n",
        "    self.mode = mode\n",
        "\n",
        "    self._get_weight()\n",
        "    self.nn1 = QuantizedLinearReLU(in_features=28*28, out_features=120, weight=self.weight_dic[0], bias=self.bias_dic[0], scale=self.scale[1], zero_point=self.zero_point[1], mode=self.mode)\n",
        "    self.nn2 = QuantizedLinearReLU(in_features=120, out_features=84, weight=self.weight_dic[1], bias=self.bias_dic[1], scale=self.scale[2], zero_point=self.zero_point[2], mode=self.mode)\n",
        "    self.nn3 = QuantizedLinear(in_features=84, out_features=10, weight=self.weight_dic[2], bias=self.bias_dic[2], scale=self.scale[3], zero_point=self.zero_point[3], mode=self.mode)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 28 * 28)\n",
        "    x = Quantize_per_tensor(x, self.scale[0], self.zero_point[0], mode=self.mode).repr()\n",
        "    x = self.nn1(x)\n",
        "    x = self.nn2(x)\n",
        "    x = self.nn3(x)\n",
        "    x = x.dequantize()\n",
        "    return x\n",
        "\n",
        "  def _get_weight(self):\n",
        "    for name, paras in model.named_parameters():\n",
        "      if \"weight\" in name:\n",
        "        self.weight_dic.append(paras)\n",
        "      elif \"bias\" in name:\n",
        "        self.bias_dic.append(paras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHIbDajdAy1J"
      },
      "source": [
        "# Normal quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "f9L2a2wUlhmy"
      },
      "outputs": [],
      "source": [
        "model = copy.deepcopy(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zVHUCwHh9sSj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00784313725490196, 0.025387292749741497, 0.025755852343989354, 0.0779837963627834]\n",
            "[0, -128, -128, -20]\n"
          ]
        }
      ],
      "source": [
        "scale_dic = []\n",
        "zero_dic = []\n",
        "\n",
        "#Calibrate to compute s、z of all layer at the same time\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for node in ['x', 'relu', 'relu_1', 'nn3']:\n",
        "    extractor = feature_extraction.create_feature_extractor(model, [node]).cpu()\n",
        "    output = extractor(input)[node]\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    scale = (max_val - min_val) / (q_max - q_min)\n",
        "    zero = round(q_min - min_val / scale)\n",
        "    q = Quantize_per_tensor(output, scale=scale, zero_point=zero, mode=\"normal\")\n",
        "    scale_dic.append(scale)\n",
        "    zero_dic.append(zero)\n",
        "  break\n",
        "\n",
        "print(scale_dic)\n",
        "print(zero_dic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tpEEanUq8HQH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.025387292749741497, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.025755852343989354, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.0779837963627834, zero_point=-20)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "Quantized_normal_model = QuantizedModel(model, scale_dic, zero_dic, mode=\"normal\")\n",
        "print(Quantized_normal_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aEedHo2BFHy"
      },
      "source": [
        "# Clip quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "InrJuJoiBD0u"
      },
      "outputs": [],
      "source": [
        "model = copy.deepcopy(FP32_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "8keSxS7xBUVq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00784313725490196, 0.023851540509392234, 0.02765337626139323, 0.09132448832194011]\n",
            "[0, -128, -128, -31]\n"
          ]
        }
      ],
      "source": [
        "scale_dic = []\n",
        "zero_dic = []\n",
        "\n",
        "#Calibrate to compute s、z of all layer at the same time\n",
        "for batch in train_loader:\n",
        "  input, label = batch\n",
        "  for node in ['x', 'relu', 'relu_1', 'nn3']:\n",
        "    extractor = feature_extraction.create_feature_extractor(model, [node]).cpu()\n",
        "    output = extractor(input)[node]\n",
        "    q_min, q_max = -128, 127\n",
        "    min_val, max_val = np.min(output.detach().numpy()), np.max(output.detach().numpy())\n",
        "    scale = (max_val - min_val) / (q_max - q_min)\n",
        "    zero = round(q_min - min_val / scale)\n",
        "    q = Quantize_per_tensor(output, scale=scale, zero_point=zero, mode=\"clip\")\n",
        "    scale_dic.append(scale)\n",
        "    zero_dic.append(zero)\n",
        "  break\n",
        "\n",
        "print(scale_dic)\n",
        "print(zero_dic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Rq8CHZEgBXZh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QuantizedModel(\n",
            "  (nn1): QuantizedLinearReLU(in_features=784, out_features=120, scale=0.023851540509392234, zero_point=-128)\n",
            "  (nn2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.02765337626139323, zero_point=-128)\n",
            "  (nn3): QuantizedLinear(in_features=84, out_features=10, scale=0.09132448832194011, zero_point=-31)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "Quantized_clip_model = QuantizedModel(model, scale_dic, zero_dic, mode=\"clip\")\n",
        "print(Quantized_clip_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3aQIb5Ln8toE"
      },
      "outputs": [],
      "source": [
        "#define evaluate function\n",
        "def Evaluate(model, loader):\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data in loader:\n",
        "      images, labels = data\n",
        "      outputs = model(images)\n",
        "      # the class with the highest energy is what we choose as prediction\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "  test_loss = 0\n",
        "\n",
        "  print(\"========================================= PERFORMANCE =============================================\")\n",
        "  print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "04PMmzZSBwUp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "\n",
            "Accuracy: 8375/10000 (84%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Normal quantize\n",
        "Evaluate(Quantized_normal_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6giaE2PoB6Zd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================= PERFORMANCE =============================================\n",
            "\n",
            "Accuracy: 8373/10000 (84%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Clip quantize\n",
        "Evaluate(Quantized_clip_model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
